{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "ZpjbPIx1zOnM",
        "outputId": "234e6b52-3e56-42b3-e8a7-add8454b41cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (\\\\( Y \\\\)) and an independent variable (\\\\( X \\\\)) using a straight-line equation: \\\\( Y = β_0 + β_1X + ε \\\\). Here, \\\\( β_0 \\\\) is the intercept, \\\\( β_1 \\\\) is the slope, and \\\\( ε \\\\) is the error term. It assumes a linear relationship, independence of observations, constant variance of errors (homoscedasticity), and normally distributed residuals. This technique is widely used for predictions, \\n  such as estimating sales based on advertising spend or house prices based on square footage.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''Q1 What is Simple Linear Regression.'''\n",
        "\n",
        "'''Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (\\( Y \\)) and an independent variable (\\( X \\)) using a straight-line equation: \\( Y = β_0 + β_1X + ε \\). Here, \\( β_0 \\) is the intercept, \\( β_1 \\) is the slope, and \\( ε \\) is the error term. It assumes a linear relationship, independence of observations, constant variance of errors (homoscedasticity), and normally distributed residuals. This technique is widely used for predictions,\n",
        "  such as estimating sales based on advertising spend or house prices based on square footage.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2 What are the key assumptions of Simple Linear Regression.'''\n",
        "\n",
        "'''Simple Linear Regression relies on four key assumptions: **Linearity**, meaning the relationship between the independent (\\(X\\)) and dependent (\\(Y\\)) variables is a straight line; **Independence**, ensuring that observations are not correlated with each other; **Homoscedasticity**, where the variance of residuals (errors) remains constant across all values of \\(X\\); and **Normality**, requiring that the residuals follow a normal distribution. Violating these assumptions can lead to inaccurate predictions and unreliable model performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "4VS05okVz8My",
        "outputId": "3df31e5f-9ccc-41fa-fbd8-b49b56763f0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Simple Linear Regression relies on four key assumptions: **Linearity**, meaning the relationship between the independent (\\\\(X\\\\)) and dependent (\\\\(Y\\\\)) variables is a straight line; **Independence**, ensuring that observations are not correlated with each other; **Homoscedasticity**, where the variance of residuals (errors) remains constant across all values of \\\\(X\\\\); and **Normality**, requiring that the residuals follow a normal distribution. Violating these assumptions can lead to inaccurate predictions and unreliable model performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3 What does the coefficient m represent in the equation Y=mX+c.'''\n",
        "\n",
        "''' It represents Slope.'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1i8ccFjP0XYZ",
        "outputId": "ca8ef5f7-6578-4dc6-9fc2-60afd0a7f081"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' It represents Slope.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4 What does the intercept c represent in the equation Y=mX+c.'''\n",
        "\n",
        "'''It represents the point where line intersects Y-axis.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cc4KcCiE00vf",
        "outputId": "52fb7d10-bf1e-4bf7-8938-5b4cffee48fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It represents the point where line intersects Y-axis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5 How do we calculate the slope m in Simple Linear Regression.'''\n",
        "\n",
        "''' By taking ratio of difference of y values and x values.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QUPbOzgP1Cd8",
        "outputId": "497b6ef8-7479-4b8a-f715-e3c47684dc50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' By taking ratio of difference of y values and x values.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6 What is the purpose of the least squares method in Simple Linear Regression.'''\n",
        "\n",
        "'''The **least squares method** in Simple Linear Regression is used to find the best-fitting line by minimizing the sum of the squared differences between the actual and predicted values of the dependent variable (\\(Y\\)). It calculates the optimal values of the slope (\\(β_1\\)) and intercept (\\(β_0\\)) by reducing the overall error, ensuring the line is as close as possible to the data points. This method improves prediction accuracy and provides the most reliable linear relationship between the independent (\\(X\\)) and dependent (\\(Y\\)) variables.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "_3UhuuTM1T7K",
        "outputId": "20fdd207-8416-4236-ea8f-471f11c34d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The **least squares method** in Simple Linear Regression is used to find the best-fitting line by minimizing the sum of the squared differences between the actual and predicted values of the dependent variable (\\\\(Y\\\\)). It calculates the optimal values of the slope (\\\\(β_1\\\\)) and intercept (\\\\(β_0\\\\)) by reducing the overall error, ensuring the line is as close as possible to the data points. This method improves prediction accuracy and provides the most reliable linear relationship between the independent (\\\\(X\\\\)) and dependent (\\\\(Y\\\\)) variables.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression.'''\n",
        "\n",
        "'''Higher the R2 value higher the correleation between dependent and independent variable.'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6wxQYceA1m_K",
        "outputId": "56fa058e-5718-49dd-9305-73e8d8976966"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Higher the R2 value higher the correleation between dependent and independent variable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8 What is Multiple Linear Regression.'''\n",
        "\n",
        "'''**Multiple Linear Regression (MLR)** is an extension of Simple Linear Regression that models the relationship between one dependent variable (\\( Y \\)) and multiple independent variables (\\( X_1, X_2, ..., X_n \\)). It helps in predicting outcomes based on multiple factors by fitting a linear equation to the data: \\( Y = β_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ε \\). This method is widely used in fields like finance, marketing, and healthcare to analyze how multiple variables influence a particular outcome, improving prediction accuracy and decision-making.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "msixPtKe13jc",
        "outputId": "4d8522e4-1842-4718-b743-0f989d28db90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Multiple Linear Regression (MLR)** is an extension of Simple Linear Regression that models the relationship between one dependent variable (\\\\( Y \\\\)) and multiple independent variables (\\\\( X_1, X_2, ..., X_n \\\\)). It helps in predicting outcomes based on multiple factors by fitting a linear equation to the data: \\\\( Y = β_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ε \\\\). This method is widely used in fields like finance, marketing, and healthcare to analyze how multiple variables influence a particular outcome, improving prediction accuracy and decision-making.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9 What is the main difference between Simple and Multiple Linear Regression.'''\n",
        "\n",
        "'''Only one independent variable in SLR and multiple independent variables in MLR.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tugo0YNk2M8o",
        "outputId": "643fb0ca-d0df-43da-d666-1690cb79821c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Only one independent variable in SLR and multiple independent variables in MLR.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10 What are the key assumptions of Multiple Linear Regression.'''\n",
        "\n",
        "'''Multiple Linear Regression (MLR)relies on several key assumptions for accurate predictions. **Linearity** assumes a linear relationship between the independent variables (\\(X_1, X_2, ...\\)) and the dependent variable (\\(Y\\)). **Independence** ensures that observations are not correlated. **Homoscedasticity** means the variance of residuals (errors) remains constant across all values of \\(X\\). **Normality** assumes that residuals follow a normal distribution. **No multicollinearity** ensures that independent variables are not highly correlated with each other, preventing unreliable coefficient estimates. Violating these assumptions can lead to biased results and poor model performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "5nqolQcu2dGq",
        "outputId": "a6f50926-8d84-4bd4-8dbd-503549329478"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multiple Linear Regression (MLR)relies on several key assumptions for accurate predictions. **Linearity** assumes a linear relationship between the independent variables (\\\\(X_1, X_2, ...\\\\)) and the dependent variable (\\\\(Y\\\\)). **Independence** ensures that observations are not correlated. **Homoscedasticity** means the variance of residuals (errors) remains constant across all values of \\\\(X\\\\). **Normality** assumes that residuals follow a normal distribution. **No multicollinearity** ensures that independent variables are not highly correlated with each other, preventing unreliable coefficient estimates. Violating these assumptions can lead to biased results and poor model performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model.'''\n",
        "\n",
        "'''Heteroscedasticity** occurs when the variance of residuals (errors) in a Multiple Linear Regression model is not constant across all levels of the independent variables. This violates the assumption of **homoscedasticity**, leading to unreliable statistical inferences. When heteroscedasticity is present, the model’s predictions may still be unbiased, but the standard errors of the coefficients become inaccurate, affecting hypothesis tests and confidence intervals. This can result in misleading significance levels and inefficient estimators, making the model less reliable for decision-making. Detecting and correcting heteroscedasticity improves the accuracy and interpretability of regression results.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "dm7VO9H62zHG",
        "outputId": "b8514365-373c-4f82-d67f-72f422c7f618"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Heteroscedasticity** occurs when the variance of residuals (errors) in a Multiple Linear Regression model is not constant across all levels of the independent variables. This violates the assumption of **homoscedasticity**, leading to unreliable statistical inferences. When heteroscedasticity is present, the model’s predictions may still be unbiased, but the standard errors of the coefficients become inaccurate, affecting hypothesis tests and confidence intervals. This can result in misleading significance levels and inefficient estimators, making the model less reliable for decision-making. Detecting and correcting heteroscedasticity improves the accuracy and interpretability of regression results.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12 How can you improve a Multiple Linear Regression model with high multicollinearity.'''\n",
        "\n",
        "'''To improve a **Multiple Linear Regression** model with **high multicollinearity**, you can take several approaches. First, **remove highly correlated independent variables** to reduce redundancy. Second, use **Variance Inflation Factor (VIF)** to identify and eliminate problematic variables. Third, apply **Principal Component Analysis (PCA)** or **Dimensionality Reduction** techniques to transform correlated variables into uncorrelated ones. Fourth, consider **Ridge Regression or Lasso Regression**, which apply regularization to shrink coefficients and reduce multicollinearity. By addressing multicollinearity, you improve model stability, enhance interpretability, and ensure more reliable predictions.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "7RKUmgPt3FlM",
        "outputId": "2ea50cd6-9d59-4649-e7cc-287af57153ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To improve a **Multiple Linear Regression** model with **high multicollinearity**, you can take several approaches. First, **remove highly correlated independent variables** to reduce redundancy. Second, use **Variance Inflation Factor (VIF)** to identify and eliminate problematic variables. Third, apply **Principal Component Analysis (PCA)** or **Dimensionality Reduction** techniques to transform correlated variables into uncorrelated ones. Fourth, consider **Ridge Regression or Lasso Regression**, which apply regularization to shrink coefficients and reduce multicollinearity. By addressing multicollinearity, you improve model stability, enhance interpretability, and ensure more reliable predictions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13 What are some common techniques for transforming categorical variables for use in regression models.'''\n",
        "\n",
        "'''To use **categorical variables** in regression models, they must be transformed into numerical formats. Common techniques include **One-Hot Encoding**, which creates binary columns for each category, and **Label Encoding**, which assigns a unique integer to each category (best for ordinal data). **Ordinal Encoding** is useful when categories have a natural order, such as \"Low,\" \"Medium,\" and \"High.\" **Target Encoding** replaces categories with the mean of the target variable for each category, often used in high-cardinality data. Choosing the right technique ensures the model correctly interprets categorical data and improves prediction accuracy.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "V1V-bEf44ReZ",
        "outputId": "02e8e6cb-e2ba-4209-f9c8-087ae0cb8e78"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To use **categorical variables** in regression models, they must be transformed into numerical formats. Common techniques include **One-Hot Encoding**, which creates binary columns for each category, and **Label Encoding**, which assigns a unique integer to each category (best for ordinal data). **Ordinal Encoding** is useful when categories have a natural order, such as \"Low,\" \"Medium,\" and \"High.\" **Target Encoding** replaces categories with the mean of the target variable for each category, often used in high-cardinality data. Choosing the right technique ensures the model correctly interprets categorical data and improves prediction accuracy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q14 What is the role of interaction terms in Multiple Linear Regression.'''\n",
        "\n",
        "'''**Interaction terms** in Multiple Linear Regression represent the combined effect of two or more independent variables on the dependent variable. They allow the model to capture relationships where the effect of one variable depends on the level of another. For example, in a model predicting sales, the effect of advertising spend might vary depending on the region. By including interaction terms (e.g., \\(X_1 \\times X_2\\)), the model can better account for these complex relationships, leading to more accurate predictions and a deeper understanding of how variables influence the outcome together.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KmHkmzlD4fm3",
        "outputId": "adb7da45-ec45-4675-ce8b-4456a6574340"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Interaction terms** in Multiple Linear Regression represent the combined effect of two or more independent variables on the dependent variable. They allow the model to capture relationships where the effect of one variable depends on the level of another. For example, in a model predicting sales, the effect of advertising spend might vary depending on the region. By including interaction terms (e.g., \\\\(X_1 \\times X_2\\\\)), the model can better account for these complex relationships, leading to more accurate predictions and a deeper understanding of how variables influence the outcome together.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression.'''\n",
        "\n",
        "'''In **Simple Linear Regression**, the intercept represents the predicted value of the dependent variable when the independent variable is zero. It’s the point where the regression line crosses the \\( Y \\)-axis. In **Multiple Linear Regression**, the intercept represents the predicted value of the dependent variable when all independent variables are zero. However, the interpretation of the intercept in MLR is more complex, as it assumes all predictors are at their baseline values, which might not be meaningful or realistic in certain cases, especially if some predictors cannot actually take a value of zero.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "cPIncJev5Ap7",
        "outputId": "92f2347c-c6d1-42e0-822f-a17cbf924473"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In **Simple Linear Regression**, the intercept represents the predicted value of the dependent variable when the independent variable is zero. It’s the point where the regression line crosses the \\\\( Y \\\\)-axis. In **Multiple Linear Regression**, the intercept represents the predicted value of the dependent variable when all independent variables are zero. However, the interpretation of the intercept in MLR is more complex, as it assumes all predictors are at their baseline values, which might not be meaningful or realistic in certain cases, especially if some predictors cannot actually take a value of zero.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q16 What is the significance of the slope in regression analysis, and how does it affect predictions.'''\n",
        "\n",
        "'''The **slope** in regression analysis represents the change in the dependent variable for a one-unit change in the independent variable. In **Simple Linear Regression**, it indicates how much the predicted value of \\( Y \\) increases or decreases as \\( X \\) increases by one unit. In **Multiple Linear Regression**, each slope corresponds to the effect of a specific independent variable, holding other variables constant. A larger absolute slope suggests a stronger effect, while a slope close to zero implies a weak or no effect. The slope directly influences predictions, as it helps quantify how changes in predictors drive changes in the outcome.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xdUGxYjS6n_D",
        "outputId": "9ba8ba7c-6c93-4880-8acc-d3989f8fa3c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The **slope** in regression analysis represents the change in the dependent variable for a one-unit change in the independent variable. In **Simple Linear Regression**, it indicates how much the predicted value of \\\\( Y \\\\) increases or decreases as \\\\( X \\\\) increases by one unit. In **Multiple Linear Regression**, each slope corresponds to the effect of a specific independent variable, holding other variables constant. A larger absolute slope suggests a stronger effect, while a slope close to zero implies a weak or no effect. The slope directly influences predictions, as it helps quantify how changes in predictors drive changes in the outcome.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q17 How does the intercept in a regression model provide context for the relationship between variables.'''\n",
        "\n",
        "'''The **intercept** in a regression model provides the baseline value of the dependent variable when all independent variables are zero. It gives context for the relationship between variables by representing the starting point or reference value of the outcome before considering the effects of predictors. While the intercept itself may not always be meaningful (especially when predictors cannot realistically be zero), it helps anchor the model and can be interpreted as the expected outcome in the absence of the independent variables' influence. In multiple regression, it shows the predicted value when all predictors are at their baseline or reference levels.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "t6yfKFL87LFQ",
        "outputId": "a0d35c0c-0fc3-4c6d-fc07-4ba513fa2eb7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The **intercept** in a regression model provides the baseline value of the dependent variable when all independent variables are zero. It gives context for the relationship between variables by representing the starting point or reference value of the outcome before considering the effects of predictors. While the intercept itself may not always be meaningful (especially when predictors cannot realistically be zero), it helps anchor the model and can be interpreted as the expected outcome in the absence of the independent variables' influence. In multiple regression, it shows the predicted value when all predictors are at their baseline or reference levels.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q18 What are the limitations of using R² as a sole measure of model performance.'''\n",
        "\n",
        "'''Using **R²** as the sole measure of model performance has several limitations. While it indicates the proportion of variance in the dependent variable explained by the model, it doesn't account for the complexity or overfitting of the model. R² can increase with the inclusion of more predictors, even if they are not meaningful, potentially misleading model interpretation. It also doesn’t reveal whether the model’s assumptions, such as linearity or homoscedasticity, are met. Therefore, R² should be used alongside other metrics like **Adjusted R²**, **RMSE (Root Mean Squared Error)**, and residual analysis to get a more accurate assessment of model performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "JCcoGDP37YPx",
        "outputId": "2f11a1bc-55a8-426e-d867-c3ea967aa06c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Using **R²** as the sole measure of model performance has several limitations. While it indicates the proportion of variance in the dependent variable explained by the model, it doesn't account for the complexity or overfitting of the model. R² can increase with the inclusion of more predictors, even if they are not meaningful, potentially misleading model interpretation. It also doesn’t reveal whether the model’s assumptions, such as linearity or homoscedasticity, are met. Therefore, R² should be used alongside other metrics like **Adjusted R²**, **RMSE (Root Mean Squared Error)**, and residual analysis to get a more accurate assessment of model performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q19 How would you interpret a large standard error for a regression coefficient.'''\n",
        "\n",
        "'''A **large standard error** for a regression coefficient indicates that the estimate of the coefficient is highly uncertain, meaning the model’s prediction for that variable is not precise. It suggests that the variable may not be a strong predictor of the dependent variable, or there could be issues like multicollinearity, where independent variables are highly correlated, making it difficult to isolate their individual effects. A large standard error reduces the statistical significance of the coefficient, potentially leading to unreliable conclusions about the relationship between the predictor and the outcome.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MBeLYtI47j6m",
        "outputId": "06ac9e4f-ad53-4ba4-cf33-c3cb4145e2a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A **large standard error** for a regression coefficient indicates that the estimate of the coefficient is highly uncertain, meaning the model’s prediction for that variable is not precise. It suggests that the variable may not be a strong predictor of the dependent variable, or there could be issues like multicollinearity, where independent variables are highly correlated, making it difficult to isolate their individual effects. A large standard error reduces the statistical significance of the coefficient, potentially leading to unreliable conclusions about the relationship between the predictor and the outcome.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q20 How can heteroscedasticity be identified in residual plots, and why is it important to address it.'''\n",
        "\n",
        "'''**Heteroscedasticity** can be identified in residual plots by looking for patterns in the spread of residuals as a function of the predicted values or independent variables. If the spread of residuals increases or decreases systematically with the predicted values (forming a cone or funnel shape), it suggests heteroscedasticity. This is important to address because it violates the assumption of constant variance in regression, leading to inefficient estimates and unreliable statistical tests. Correcting heteroscedasticity ensures more accurate predictions, valid hypothesis testing, and reliable model performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "53Rdh3O57ukY",
        "outputId": "0f829b84-1239-4a02-cf90-61766d83aa4a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Heteroscedasticity** can be identified in residual plots by looking for patterns in the spread of residuals as a function of the predicted values or independent variables. If the spread of residuals increases or decreases systematically with the predicted values (forming a cone or funnel shape), it suggests heteroscedasticity. This is important to address because it violates the assumption of constant variance in regression, leading to inefficient estimates and unreliable statistical tests. Correcting heteroscedasticity ensures more accurate predictions, valid hypothesis testing, and reliable model performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R².'''\n",
        "\n",
        "'''If a **Multiple Linear Regression** model has a high **R²** but a low **Adjusted R²**, it suggests that while the model explains a large proportion of the variance in the dependent variable, the addition of predictors may not be improving the model in a meaningful way. The high R² could be due to overfitting, where irrelevant or redundant predictors are included. **Adjusted R²** penalizes the inclusion of unnecessary variables and provides a more accurate measure of model fit, so a large difference between R² and Adjusted R² indicates that the model may be overly complex and not generalizing well to new data.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "h4akX_iA8BHr",
        "outputId": "7805853a-dedc-44a7-80ce-e9dfc49cdea6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'If a **Multiple Linear Regression** model has a high **R²** but a low **Adjusted R²**, it suggests that while the model explains a large proportion of the variance in the dependent variable, the addition of predictors may not be improving the model in a meaningful way. The high R² could be due to overfitting, where irrelevant or redundant predictors are included. **Adjusted R²** penalizes the inclusion of unnecessary variables and provides a more accurate measure of model fit, so a large difference between R² and Adjusted R² indicates that the model may be overly complex and not generalizing well to new data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q22 Why is it important to scale variables in Multiple Linear Regression.'''\n",
        "\n",
        "'''Scaling variables in Multiple Linear Regression is important because it ensures that all independent variables are on a similar scale, preventing one variable from dominating the model due to its larger range. This is especially crucial when the variables have different units (e.g., age in years vs. income in thousands). Scaling improves the interpretation of coefficients, enhances numerical stability, and helps regularization techniques (like Ridge or Lasso) perform better. It also aids in detecting multicollinearity and ensures that the model’s performance is not biased by the magnitude of the predictors.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "JJUnLuBl8Sa1",
        "outputId": "f50201c8-ffc5-4b37-a8cf-b173ac30c47a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Scaling variables in Multiple Linear Regression is important because it ensures that all independent variables are on a similar scale, preventing one variable from dominating the model due to its larger range. This is especially crucial when the variables have different units (e.g., age in years vs. income in thousands). Scaling improves the interpretation of coefficients, enhances numerical stability, and helps regularization techniques (like Ridge or Lasso) perform better. It also aids in detecting multicollinearity and ensures that the model’s performance is not biased by the magnitude of the predictors.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q23 What is polynomial regression.'''\n",
        "\n",
        "'''**Polynomial Regression** is a form of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. Instead of fitting a straight line, polynomial regression fits a curve to the data, allowing it to capture non-linear relationships. The model is expressed as:\n",
        "\n",
        "\\[\n",
        "Y = β_0 + β_1X + β_2X^2 + ... + β_nX^n + ε\n",
        "\\]\n",
        "\n",
        "Where \\( Y \\) is the dependent variable, \\( X \\) is the independent variable, and the \\( β \\)'s are the coefficients. Polynomial regression is useful when the relationship between variables is more complex than linear but should be used carefully to avoid overfitting by choosing an appropriate degree for the polynomial.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "n79Fdeir8qsQ",
        "outputId": "866cbdd2-1d3d-460f-edaf-0e3b21299280"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**Polynomial Regression** is a form of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. Instead of fitting a straight line, polynomial regression fits a curve to the data, allowing it to capture non-linear relationships. The model is expressed as:\\n\\n\\\\[\\nY = β_0 + β_1X + β_2X^2 + ... + β_nX^n + ε\\n\\\\]\\n\\nWhere \\\\( Y \\\\) is the dependent variable, \\\\( X \\\\) is the independent variable, and the \\\\( β \\\\)'s are the coefficients. Polynomial regression is useful when the relationship between variables is more complex than linear but should be used carefully to avoid overfitting by choosing an appropriate degree for the polynomial.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q24 How does polynomial regression differ from linear regression.'''\n",
        "\n",
        "'''**Polynomial Regression** differs from **Linear Regression** in the way it models the relationship between the independent and dependent variables. While linear regression fits a straight line to the data, polynomial regression fits a curve by using higher-degree powers of the independent variable (e.g., \\( X^2, X^3 \\)) in the model. This allows polynomial regression to capture non-linear relationships, whereas linear regression assumes a constant linear relationship. As a result, polynomial regression can provide more flexibility to model complex patterns in the data, but it also carries a risk of overfitting if the polynomial degree is too high.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "aUNimTZ183gz",
        "outputId": "a11d99e3-2fc0-4c24-9344-a3619e6986e1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Polynomial Regression** differs from **Linear Regression** in the way it models the relationship between the independent and dependent variables. While linear regression fits a straight line to the data, polynomial regression fits a curve by using higher-degree powers of the independent variable (e.g., \\\\( X^2, X^3 \\\\)) in the model. This allows polynomial regression to capture non-linear relationships, whereas linear regression assumes a constant linear relationship. As a result, polynomial regression can provide more flexibility to model complex patterns in the data, but it also carries a risk of overfitting if the polynomial degree is too high.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q25 When is polynomial regression used.'''\n",
        "\n",
        "'''**Polynomial Regression** is used when the relationship between the independent and dependent variables is non-linear and cannot be accurately captured by a straight line. It is useful when data shows curves or more complex patterns, such as U-shaped or inverted-U trends. By including higher-degree terms of the independent variable (e.g., \\(X^2, X^3\\)), polynomial regression allows for better modeling of these non-linear relationships. However, it should be used cautiously, as higher-degree polynomials can lead to overfitting, especially if the data is noisy or if the degree is chosen too large.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "DxfFRFx19RKf",
        "outputId": "60c70055-2c49-4e1c-b4ab-4b19faa1c617"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Polynomial Regression** is used when the relationship between the independent and dependent variables is non-linear and cannot be accurately captured by a straight line. It is useful when data shows curves or more complex patterns, such as U-shaped or inverted-U trends. By including higher-degree terms of the independent variable (e.g., \\\\(X^2, X^3\\\\)), polynomial regression allows for better modeling of these non-linear relationships. However, it should be used cautiously, as higher-degree polynomials can lead to overfitting, especially if the data is noisy or if the degree is chosen too large.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q26 What is the general equation for polynomial regression.'''\n",
        "\n",
        "'''The general equation for **Polynomial Regression** extends the linear regression model by including higher-degree terms of the independent variable. It is expressed as:\n",
        "\n",
        "\\[\n",
        "Y = β_0 + β_1X + β_2X^2 + β_3X^3 + ... + β_nX^n + ε\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( X \\) is the independent variable.\n",
        "- \\( β_0 \\) is the intercept.\n",
        "- \\( β_1, β_2, ... β_n \\) are the coefficients for each degree of \\( X \\).\n",
        "- \\( ε \\) is the error term.\n",
        "\n",
        "This equation allows the model to fit a curve to the data, capturing non-linear relationships between the variables by including powers of \\( X \\) (such as \\( X^2, X^3 \\), etc.).'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "t1oK6II49b8T",
        "outputId": "4dacdfca-71c3-4d76-e4db-96d40aff08f2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The general equation for **Polynomial Regression** extends the linear regression model by including higher-degree terms of the independent variable. It is expressed as:\\n\\n\\\\[\\nY = β_0 + β_1X + β_2X^2 + β_3X^3 + ... + β_nX^n + ε\\n\\\\]\\n\\nWhere:  \\n- \\\\( Y \\\\) is the dependent variable.  \\n- \\\\( X \\\\) is the independent variable.  \\n- \\\\( β_0 \\\\) is the intercept.  \\n- \\\\( β_1, β_2, ... β_n \\\\) are the coefficients for each degree of \\\\( X \\\\).  \\n- \\\\( ε \\\\) is the error term.  \\n\\nThis equation allows the model to fit a curve to the data, capturing non-linear relationships between the variables by including powers of \\\\( X \\\\) (such as \\\\( X^2, X^3 \\\\), etc.).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q27 Can polynomial regression be applied to multiple variables.'''\n",
        "\n",
        "'''Yes, **Polynomial Regression** can be applied to multiple variables, extending beyond a single independent variable. In **Multiple Polynomial Regression**, the model includes polynomial terms of multiple predictors, allowing it to capture complex relationships between the dependent variable and several independent variables. The general equation for this would involve not just higher powers of individual variables (e.g., \\(X_1^2, X_2^2\\)), but also interaction terms (e.g., \\(X_1X_2\\), \\(X_1^2X_2\\)), which account for how variables interact non-linearly. This makes the model more flexible, but also increases the risk of overfitting if too many polynomial terms are included.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "8Kb6NmT09zwj",
        "outputId": "cb076300-fb7b-4b65-d948-f78b9d698988"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, **Polynomial Regression** can be applied to multiple variables, extending beyond a single independent variable. In **Multiple Polynomial Regression**, the model includes polynomial terms of multiple predictors, allowing it to capture complex relationships between the dependent variable and several independent variables. The general equation for this would involve not just higher powers of individual variables (e.g., \\\\(X_1^2, X_2^2\\\\)), but also interaction terms (e.g., \\\\(X_1X_2\\\\), \\\\(X_1^2X_2\\\\)), which account for how variables interact non-linearly. This makes the model more flexible, but also increases the risk of overfitting if too many polynomial terms are included.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q28 What are the limitations of polynomial regression.'''\n",
        "\n",
        "'''Polynomial Regression** has several limitations. One major drawback is the risk of **overfitting**, especially when using high-degree polynomials, as the model may fit the training data too closely and fail to generalize well to new data. Polynomial regression can also be sensitive to **outliers**, which can disproportionately affect the curve. Additionally, interpreting the model becomes more complex as the degree of the polynomial increases. It also assumes that the relationship between the independent and dependent variables can be captured by polynomial terms, which may not always be appropriate for the data. Finally, higher-degree polynomials can lead to **multicollinearity**, where the predictors become highly correlated, making the model unstable.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "G1T9tnQ9-rhS",
        "outputId": "d4168f08-68fc-4dcb-ccbe-68f0c8bc27f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Polynomial Regression** has several limitations. One major drawback is the risk of **overfitting**, especially when using high-degree polynomials, as the model may fit the training data too closely and fail to generalize well to new data. Polynomial regression can also be sensitive to **outliers**, which can disproportionately affect the curve. Additionally, interpreting the model becomes more complex as the degree of the polynomial increases. It also assumes that the relationship between the independent and dependent variables can be captured by polynomial terms, which may not always be appropriate for the data. Finally, higher-degree polynomials can lead to **multicollinearity**, where the predictors become highly correlated, making the model unstable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q29 What methods can be used to evaluate model fit when selecting the degree of a polynomial.'''\n",
        "\n",
        "'''When selecting the degree of a polynomial, several methods can be used to evaluate model fit. **Cross-validation** is a common technique, where the data is split into training and validation sets to test how well the model generalizes to unseen data. **Adjusted R²** is another useful metric, as it adjusts for the number of predictors and helps avoid overfitting by penalizing complex models. Additionally, **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)** can be used to assess the model’s prediction accuracy. By comparing these metrics across different polynomial degrees, you can choose the degree that best balances model complexity and performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "un8ugTAs-8g-",
        "outputId": "c49b6b49-7517-4a8a-8c9e-8b0fe36471b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When selecting the degree of a polynomial, several methods can be used to evaluate model fit. **Cross-validation** is a common technique, where the data is split into training and validation sets to test how well the model generalizes to unseen data. **Adjusted R²** is another useful metric, as it adjusts for the number of predictors and helps avoid overfitting by penalizing complex models. Additionally, **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)** can be used to assess the model’s prediction accuracy. By comparing these metrics across different polynomial degrees, you can choose the degree that best balances model complexity and performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q30 Why is visualization important in polynomial regression.'''\n",
        "\n",
        "''' Visualization is important in **Polynomial Regression** because it helps in understanding the fit of the model and how well it captures the relationship between the independent and dependent variables. By plotting the data and the polynomial regression curve, you can visually assess whether the model is appropriately capturing the underlying patterns or if it’s overfitting (fitting the noise in the data). Visualization also aids in selecting the correct degree of the polynomial, ensuring that the curve aligns with the data without being too complex or too simple. It provides an intuitive way to evaluate model performance and make adjustments as needed.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Eco0rGqx_qId",
        "outputId": "4c163832-bfce-4413-e6d8-8d607a1dbb3c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Visualization is important in **Polynomial Regression** because it helps in understanding the fit of the model and how well it captures the relationship between the independent and dependent variables. By plotting the data and the polynomial regression curve, you can visually assess whether the model is appropriately capturing the underlying patterns or if it’s overfitting (fitting the noise in the data). Visualization also aids in selecting the correct degree of the polynomial, ensuring that the curve aligns with the data without being too complex or too simple. It provides an intuitive way to evaluate model performance and make adjustments as needed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q31 - How is polynomial regression implemented in Python.'''\n",
        "\n",
        "'''Polynomial regression in Python can be implemented using libraries like NumPy and Scikit-learn.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YYoYbDhB_3Pr",
        "outputId": "efa533f1-db62-4e4f-95dd-ecc803e32600"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Polynomial regression in Python can be implemented using libraries like NumPy and Scikit-learn.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dbO7msIAE2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}